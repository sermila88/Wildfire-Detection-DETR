#!/bin/bash
#SBATCH --job-name=rt_detr_hyperparameter_tuning
#SBATCH --output=outputs/rtdetr_official_hyperparameter_tuning_v1/logs/rt_detr_hyperparameter_tuning_%j.out
#SBATCH --error=outputs/rtdetr_official_hyperparameter_tuning_v1/logs/rt_detr_hyperparameter_tuning_%j.err
#SBATCH --gres=gpu:1
#SBATCH --partition=gpgpu,a16gpu,gpgpuB
#SBATCH --mem=16G


echo "ğŸ”¥ RT-DETR Hyperparameter Tuning (Official Implementation)"
echo "ğŸ• Job started at: $(date)"
echo "ğŸ–¥ï¸  Running on node: $(hostname)"

# Create logs directory for this experiment
mkdir -p outputs/rtdetr_hyperparameter_tuning_v1/logs

echo "ğŸ”§ GPU info:"
nvidia-smi

source /vol/bitbucket/${USER}/rf-detr-wildfire/.venv/bin/activate
source /vol/cuda/12.0.0/setup.sh
/usr/bin/nvidia-smi

# Change to project directory  
cd /vol/bitbucket/si324/rf-detr-wildfire

# Set Ultralytics cache to project directory to avoid disk quota issues
export ULTRALYTICS_CACHE_DIR=/vol/bitbucket/si324/rf-detr-wildfire/.cache

# Create outputs directory
mkdir -p outputs/rtdetr_hyperparameter_tuning_v1

echo ""
echo "ğŸ¯ RT-DETR Optimization Configuration:"
echo "   ğŸ“Š Trials: 20 systematic trials with TPE sampler + Median pruner"
echo "   ğŸ—ï¸  Model: RT-DETR-X (54.8 mAP, best accuracy for tiny objects)"
echo "   ğŸ”§ Parameters: lr0 (5e-5 to 5e-4), weight_decay, batch_size, epochs (36-108)"
echo "   âš–ï¸  Loss weights: cls (1.0-4.0), bbox (2.0-8.0), giou (1.0-3.0)"
echo "   ğŸ¨ Augmentation: mixup (0.0-0.2), mosaic (0.8-1.0), copy_paste (0.1)"
echo "   ğŸ“ Resolution range: 640-1024px (RT-DETR standards)"
echo "   ğŸ“¦ Batch size: 4-32 (official effective batch size range)"
echo "   ğŸš€ Optimizer: AdamW (primary) + SGD with cosine LR scheduling"
echo "   âš¡ Early stopping: 20-50 patience with median pruning"
echo "   ğŸ“ˆ Evaluation: mAP50-95 (COCO standard metric)"
echo "   ğŸ› ï¸  Schedule: 3x-9x epochs (36-108, based on 6x=72 official)"
echo "   ğŸ¯ Focal loss: alpha (0.2-0.3), gamma (1.5-2.5)"
echo "   ğŸ“Š Logging: Complete trial metadata, checkpoints, metrics, backups"
echo "   ğŸ’¾ Storage: ALL trials preserved with full resumability"
echo "   ğŸ”„ Backups: Study database + trial states backed up continuously"
echo "   â±ï¸  Expected duration: 60-96 hours (comprehensive evaluation)"
echo "   ğŸ›¡ï¸  Resumability: Enterprise-grade state preservation for 4-day runs"
echo "   ğŸ“‹ Standards: Following official RT-DETR paper & repository guidelines"

echo ""
echo "ğŸš€ Starting RT-DETR hyperparameter optimization "

# Run RT-DETR hyperparameter optimization
python train/rt_detr_hyperparameter_tuning.py

echo ""
echo "âœ… RT-DETR hyperparameter optimization completed at: $(date)"
echo "ğŸ“ Results location:"
echo "   ğŸ“Š Results: outputs/rtdetr_hyperparameter_tuning_v1/comprehensive_final_results.json"
echo "   ğŸ“‚ Individual trials: outputs/rtdetr_hyperparameter_tuning_v1/trial_*/"
echo "   ğŸ’¾ Optuna database: outputs/rtdetr_hyperparameter_tuning_v1/study.db"
echo "   ğŸ”„ Database backups: outputs/rtdetr_hyperparameter_tuning_v1/backups/"
echo "   ğŸ“ˆ Per-trial data: checkpoints/, plots/, metrics/, logs/, backups/"
echo "   ğŸ† Best model: outputs/rtdetr_hyperparameter_tuning_v1/trial_XXX/checkpoints/weights/best.pt"
echo ""
echo "ğŸ¯ Check comprehensive_final_results.json for optimal RT-DETR hyperparameters!"
echo "ğŸš€ Use the best trial parameters for production wildfire detection model."
echo "ğŸ›¡ï¸  All data preserved for analysis, resumption, and model deployment!"

